{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community langchain-openai langchain-core beautifulsoup4 pypdf chromadb"
      ],
      "metadata": {
        "id": "1U3gai7PTGD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import os\n",
        "#Enter the Secret Key here!\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
      ],
      "metadata": {
        "id": "X1OLsg_bTLX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHTpNN_oS_na"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# ۱. تنظیمات اولیه\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\"\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# ۲. بارگذاری و چانک‌بندی\n",
        "loader = PyPDFLoader(\"/content/drive/MyDrive/Think-And-Grow-Rich_2011-06.pdf\")\n",
        "docs = loader.load()\n",
        "print(docs)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# ۳. ذخیره در دیتابیس برداری\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# ۴. طراحی پرامپت هوشمند با قابلیت درک تاریخچه (History)\n",
        "contextualize_q_system_prompt = (\n",
        "    \"با توجه به تاریخچه چت و آخرین سوال کاربر که ممکن است به سوالات قبلی ارجاع داشته باشد، \"\n",
        "    \"یک سوال مستقل بساز که بدون نیاز به تاریخچه قابل فهم باشد.\"\n",
        ")\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", contextualize_q_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# ۵. پرامپت پاسخ‌دهی اصلی\n",
        "qa_system_prompt = (\n",
        "    \"تو یک دستیار هوشمند هستی. از تکه‌های متن استخراج شده زیر برای پاسخ به سوال استفاده کن. \"\n",
        "    \"اگر پاسخ در متن نیست، بگو که نمی‌دانم. حداکثر در سه جمله پاسخ بده.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", qa_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# ۶. ساخت زنجیره با استفاده از LCEL (بدون استفاده از Chains قدیمی)\n",
        "# ۶. اصلاح شده: جایگزین کردن get_relevant_documents با invoke\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# زنجیره جدید و سازگار با پایتون ۳.۱۲ و لنگ‌چین جدید\n",
        "rag_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=lambda x: format_docs(retriever.invoke(x[\"input\"])) # اینجا invoke جایگزین شد\n",
        "    )\n",
        "    | qa_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# ۷. اجرای چت با حافظه\n",
        "chat_history = []\n",
        "\n",
        "print(\"--- سیستم آماده است ---\")\n",
        "while True:\n",
        "    user_input = input(\"سوال شما: \")\n",
        "    if user_input.lower() in ['exit', 'quit']: break\n",
        "\n",
        "    # اجرای زنجیره\n",
        "    response = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
        "\n",
        "    print(f\"پاسخ: {response}\")\n",
        "\n",
        "    # آپدیت حافظه\n",
        "    chat_history.append((\"human\", user_input))\n",
        "    chat_history.append((\"ai\", response))"
      ]
    }
  ]
}